@inproceedings{sawaya_flit_2017,
  Abstract   = {Understanding the extent to which computational results can
                change across platforms, compilers, and compiler flags can go a
                long way toward supporting reproducible experiments. In this
                work, we offer the first automated testing aid called FLiT
                (Floating-point Litmus Tester) that can show how much these
                results can vary for any user-given collection of computational
                kernels. Our approach is to take a collection of these kernels,
                disperse them across a collection of compute nodes (each with a
                different architecture), have them compiled and run, and bring
                the results to a central SQL database for deeper analysis.
                Properly conducting these activities requires a careful
                selection (or design) of these kernels, input generation
                methods for them, and the ability to interpret the results in
                meaningful ways. The results in this paper are meant to inform
                two different communities: (a) those interested in seeking
                higher performance by considering “IEEE unsafe” optimizations,
                but then want to understand how much result variability to
                expect, and (b) those interested in standardizing compiler
                flags and their meanings, so that one may safely port code
                across generations of compilers and architectures. By releasing
                FLiT, we have also opened up the possibility of all HPC
                developers using it as a common resource as well as
                contributing back interesting test kernels as well as best
                practices, thus extending the floating-point result-consistency
                workload we contribute.  This is the first such workload and
                result-consistency tester underlying floating-point
                reproducibility of which we are aware.}
  Author     = {Geof Sawaya and
                Michael Bentley and
                Ian Briggs and
                Ganesh Gopalakrishnan and
                Dong H. Ahn},
  pages      = {229--238},
  Location   = {Seattle},
  Month      = {Oct},
  Title      = {{FLiT: Cross-Platform Floating-Point Result-Consistency Tester
                 and Workload}},
  booktitle  = {2017 IEEE International Symposium on Workload Characterization
                (IISWC)},
  Year       = {2017},
  organization = {IEEE},
  url        = {https://doi.org/10.1109/IISWC.2017.8167780},
  doi        = {10.1109/IISWC.2017.8167780},
  isbn       = {978-1-5386-1233-0},
}

@inproceedings{bentley_flit_2019,
  Abstract   = {Successful HPC software applications are long-lived.  When
                ported across machines and their compilers, these applications
                often produce different numerical results, many of which are
                unacceptable.  Such variability is also a concern while
                optimizing the code more aggressively to gain performance.
                Efficient tools that help locate the program units (files and
                functions) within which most of the variability occurs are
                badly needed, both to plan for code ports and to root-cause
                errors due to variability when they happen in the field.  In
                this work, we offer an enhanced version of the open-source
                testing framework FLiT to serve these roles.  Key new features
                of FLiT include a suite of bisection algorithms that help
                locate the root causes of variability.  Another added feature
                allows an analysis of the tradeoffs between performance and the
                degree of variability.  Our new contributions also include a
                collection of case studies.  Results on the MFEM finite-element
                library include variability/performance tradeoffs, and the
                identification of a (hitherto unknown) abnormal level of
                result-variability even under mild compiler optimizations.
                Results from studying the Laghos proxy application include
                identifying a significantly divergent floating-point
                result-variability and successful root-causing down to the
                problematic function over as little as 14 program executions.
                Finally, in an evaluation of 4,376 controlled injections of
                floating-point perturbations on the LULESH proxy application,
                we showed that the FLiT framework has 100\% precision and recall
                in discovering the file and function locations of the
                injections all within an average of only 15 program
                executions.}
  Author     = {Michael Bentley and
                Ian Briggs and
                Ganesh Gopalakrishnan and
                Dong H. Ahn and
                Ignacio Laguna and
                Gregory L. Lee and
                Holger E. Jones},
  Title      = {{Multi-Level Analysis of Compiler-Induced Variability and
                 Performance Tradeoffs}},
  Booktitle  = {Proceedings of the 28th International Symposium on
                High-Performance Parallel and Distributed Computing},
  Series     = {HPDC '19},
  Year       = {2019},
  isbn       = {978-1-4503-6670-0},
  Location   = {Phoenix, AZ, USA},
  Pages      = {61--72},
  Numpages   = {12},
  url        = {http://doi.acm.org/10.1145/3307681.3325960},
  doi        = {10.1145/3307681.3325960},
  acmid      = {3325960},
  publisher  = {ACM},
  address    = {New York, NY, USA},
  keywords   = {code optimization,
                compilers,
                debugging,
                performance tuning,
                reproducibility},
}
